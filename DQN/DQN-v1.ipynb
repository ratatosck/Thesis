{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoInputsNet(nn.Module):\n",
    "    # ****\n",
    "    # tengo que revisar mejor sobre los channels y toda esta locura porque hay algo raro. sobretodo en la parte de los filtros.\n",
    "    \n",
    "    # revisa que todas tus dimensiones esten correctas.\n",
    "    # ****\n",
    "    def __init__(self, lr, in_channels, out_channels, kernel_size, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(TwoInputsNet, self).__init__()\n",
    "        self.conv_open = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_high = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_low = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_close = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_volume = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.fc_state = nn.Linear(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims) #input dims is the sum of all outputs of the conv layers.\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.fc3 = nn.Linear(fc2_dims, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Open High Low Close Volume in that order (also for the numpy array)\n",
    "        # input6 son dos nodos de input para saber si el agente ha comprado/vendido y en cuanto.\n",
    "    def forward(self, input1, input2, input3, input4, input5, input6):\n",
    "        c1 = self.conv_open(input1.unsqueeze(dim = 1))\n",
    "        c2 = self.conv_high(input2.unsqueeze(dim = 1))\n",
    "        c3 = self.conv_low(input3.unsqueeze(dim = 1))\n",
    "        c4 = self.conv_close(input4.unsqueeze(dim = 1))\n",
    "        c5 = self.conv_volume(input5.unsqueeze(dim = 1))\n",
    "        f1 = self.fc_state(input6)\n",
    "        \n",
    "        # now we can reshape to 2D and concat them\n",
    "        combined = T.cat((c1.view(c1.size(0), -1), \n",
    "                          c2.view(c2.size(0), -1),\n",
    "                          c3.view(c3.size(0), -1),\n",
    "                          c4.view(c4.size(0), -1),\n",
    "                          c5.view(c5.size(0), -1),\n",
    "                          f1.view(f1.size(0), -1)), dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "class Agent():\n",
    "    # ***hyperparameters\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims1, input_dims2, batch_size, n_actions,\n",
    "                max_mem_size = 100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        # self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions, input_dims=input_dims, fc1_dims=256, fc2_dims=256)\n",
    "        # (self, lr, in_channels, out_channels, kernel_size, input_dims, fc1_dims, fc2_dims, n_actions)\n",
    "        # en otras parabras (learning rate, cantidad de filtros de entrada, cantidad de filtros de salida, tamaño del kernel)\n",
    "        #  ***hyperparameters\n",
    "        self.Q_eval = TwoInputsNet(self.lr, 1, 4, 3, 562, 128, 128, n_actions)\n",
    "        #                                             ^\n",
    "        # tengo que encontrar una forma para hacer que se haga el tamaño del fc layer automaticamente\n",
    "        \n",
    "        # ***\n",
    "        # tengo que reviar si el replay memory esta funcionando correctamente, sobre todo el *input_dims\n",
    "        # as of now I checked the state_memory shape (10000, 30, 5) and it looks correct\n",
    "        # ***\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims1), dtype = np.float32)\n",
    "        self.state_memory2 = np.zeros((self.mem_size, *input_dims2), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims1), dtype=np.float32)\n",
    "        self.new_state_memory2 = np.zeros((self.mem_size, *input_dims2), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state[0]\n",
    "        self.state_memory2[index] = state[1]\n",
    "        self.new_state_memory[index] = state_[0]\n",
    "        self.new_state_memory2[index] = state_[1]\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # aqui tuve que separar la observacion en las 2 partes y transformarlas a pytorch tensor. \n",
    "            # no se si es lo mejor, pero sirve asi que...\n",
    "            \n",
    "            state = T.tensor([observation[0]]).to(self.Q_eval.device)\n",
    "            state2 = T.tensor([observation[1]]).to(self.Q_eval.device)\n",
    "#             print(state)\n",
    "            actions = self.Q_eval.forward(state[:,:,0], \n",
    "                                          state[:,:,1], \n",
    "                                          state[:,:,2], \n",
    "                                          state[:,:,3], \n",
    "                                          state[:,:,4],\n",
    "                                          state2[:])\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        state_batch2 = T.tensor(self.state_memory2[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch2 = T.tensor(self.new_state_memory2[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch[:,:,0], \n",
    "                                     state_batch[:,:,1], \n",
    "                                     state_batch[:,:,2], \n",
    "                                     state_batch[:,:,3], \n",
    "                                     state_batch[:,:,4],\n",
    "                                     state_batch2[:])[batch_index, action_batch]\n",
    "    \n",
    "        q_next = self.Q_eval.forward(new_state_batch[:,:,0], \n",
    "                                     new_state_batch[:,:,1], \n",
    "                                     new_state_batch[:,:,2], \n",
    "                                     new_state_batch[:,:,3], \n",
    "                                     new_state_batch[:,:,4],\n",
    "                                     new_state_batch2[:])\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                        else self.eps_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode ran for 4 timesteps.\n",
      "episode 0 score -0.00 average score -0.00 sum score -0.00 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 1 score 0.00 average score 0.00 sum score 0.00 epsilon 1.00\n",
      "The episode ran for 9 timesteps.\n",
      "episode 2 score -0.04 average score -0.01 sum score -0.04 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 3 score -0.00 average score -0.01 sum score -0.04 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 4 score -0.00 average score -0.01 sum score -0.04 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 5 score 0.00 average score -0.01 sum score -0.04 epsilon 1.00\n",
      "The episode ran for 4 timesteps.\n",
      "episode 6 score -0.02 average score -0.01 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 3 timesteps.\n",
      "episode 7 score -0.00 average score -0.01 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 8 score 0.00 average score -0.01 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 3 timesteps.\n",
      "episode 9 score -0.01 average score -0.01 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 3 timesteps.\n",
      "episode 10 score -0.03 average score -0.01 sum score -0.11 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 11 score -0.00 average score -0.01 sum score -0.11 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 12 score -0.00 average score -0.01 sum score -0.11 epsilon 1.00\n",
      "The episode ran for 5 timesteps.\n",
      "episode 13 score -0.01 average score -0.01 sum score -0.12 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 14 score 0.00 average score -0.01 sum score -0.12 epsilon 1.00\n",
      "The episode ran for 4 timesteps.\n",
      "episode 15 score 0.00 average score -0.01 sum score -0.11 epsilon 1.00\n",
      "The episode ran for 5 timesteps.\n",
      "episode 16 score 0.02 average score -0.01 sum score -0.09 epsilon 1.00\n",
      "The episode ran for 3 timesteps.\n",
      "episode 17 score -0.00 average score -0.01 sum score -0.10 epsilon 1.00\n",
      "The episode ran for 4 timesteps.\n",
      "episode 18 score 0.00 average score -0.00 sum score -0.09 epsilon 1.00\n",
      "The episode ran for 6 timesteps.\n",
      "episode 19 score 0.02 average score -0.00 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 2 timesteps.\n",
      "episode 20 score 0.00 average score -0.00 sum score -0.07 epsilon 1.00\n",
      "The episode ran for 11 timesteps.\n",
      "episode 21 score 0.02 average score -0.00 sum score -0.05 epsilon 0.99\n",
      "The episode ran for 4 timesteps.\n",
      "episode 22 score -0.01 average score -0.00 sum score -0.06 epsilon 0.99\n",
      "The episode ran for 6 timesteps.\n",
      "episode 23 score -0.06 average score -0.00 sum score -0.11 epsilon 0.99\n",
      "The episode ran for 6 timesteps.\n",
      "episode 24 score 0.00 average score -0.00 sum score -0.11 epsilon 0.98\n",
      "The episode ran for 14 timesteps.\n",
      "episode 25 score 0.11 average score -0.00 sum score -0.00 epsilon 0.98\n",
      "The episode ran for 6 timesteps.\n",
      "episode 26 score -0.02 average score -0.00 sum score -0.02 epsilon 0.97\n",
      "The episode ran for 3 timesteps.\n",
      "episode 27 score 0.00 average score -0.00 sum score -0.02 epsilon 0.97\n",
      "The episode ran for 8 timesteps.\n",
      "episode 28 score -0.09 average score -0.00 sum score -0.10 epsilon 0.97\n",
      "The episode ran for 8 timesteps.\n",
      "episode 29 score -0.00 average score -0.00 sum score -0.11 epsilon 0.96\n",
      "The episode ran for 2 timesteps.\n",
      "episode 30 score 0.00 average score -0.00 sum score -0.11 epsilon 0.96\n",
      "The episode ran for 4 timesteps.\n",
      "episode 31 score 0.00 average score -0.00 sum score -0.10 epsilon 0.96\n",
      "The episode ran for 3 timesteps.\n",
      "episode 32 score 0.01 average score -0.00 sum score -0.09 epsilon 0.96\n",
      "The episode ran for 2 timesteps.\n",
      "episode 33 score 0.01 average score -0.00 sum score -0.08 epsilon 0.96\n",
      "The episode ran for 4 timesteps.\n",
      "episode 34 score -0.02 average score -0.00 sum score -0.11 epsilon 0.96\n",
      "The episode ran for 7 timesteps.\n",
      "episode 35 score 0.09 average score -0.00 sum score -0.01 epsilon 0.95\n",
      "The episode ran for 3 timesteps.\n",
      "episode 36 score 0.00 average score -0.00 sum score -0.01 epsilon 0.95\n",
      "The episode ran for 4 timesteps.\n",
      "episode 37 score 0.01 average score -0.00 sum score -0.01 epsilon 0.95\n",
      "The episode ran for 2 timesteps.\n",
      "episode 38 score -0.00 average score -0.00 sum score -0.01 epsilon 0.95\n",
      "The episode ran for 2 timesteps.\n",
      "episode 39 score 0.00 average score -0.00 sum score -0.00 epsilon 0.95\n",
      "The episode ran for 7 timesteps.\n",
      "episode 40 score 0.01 average score 0.00 sum score 0.00 epsilon 0.94\n",
      "The episode ran for 8 timesteps.\n",
      "episode 41 score 0.01 average score 0.00 sum score 0.01 epsilon 0.94\n",
      "The episode ran for 3 timesteps.\n",
      "episode 42 score 0.00 average score 0.00 sum score 0.01 epsilon 0.94\n",
      "The episode ran for 4 timesteps.\n",
      "episode 43 score 0.00 average score 0.00 sum score 0.02 epsilon 0.94\n",
      "The episode ran for 3 timesteps.\n",
      "episode 44 score 0.01 average score 0.00 sum score 0.03 epsilon 0.93\n",
      "The episode ran for 5 timesteps.\n",
      "episode 45 score 0.00 average score 0.00 sum score 0.03 epsilon 0.93\n",
      "The episode ran for 4 timesteps.\n",
      "episode 46 score -0.00 average score 0.00 sum score 0.03 epsilon 0.93\n",
      "The episode ran for 5 timesteps.\n",
      "episode 47 score -0.00 average score 0.00 sum score 0.03 epsilon 0.93\n",
      "The episode ran for 6 timesteps.\n",
      "episode 48 score -0.01 average score 0.00 sum score 0.02 epsilon 0.92\n",
      "The episode ran for 3 timesteps.\n",
      "episode 49 score -0.00 average score 0.00 sum score 0.02 epsilon 0.92\n",
      "The episode ran for 2 timesteps.\n",
      "episode 50 score 0.00 average score 0.00 sum score 0.02 epsilon 0.92\n",
      "The episode ran for 5 timesteps.\n",
      "episode 51 score 0.02 average score 0.00 sum score 0.04 epsilon 0.92\n",
      "The episode ran for 3 timesteps.\n",
      "episode 52 score -0.00 average score 0.00 sum score 0.03 epsilon 0.92\n",
      "The episode ran for 4 timesteps.\n",
      "episode 53 score 0.01 average score 0.00 sum score 0.04 epsilon 0.92\n",
      "The episode ran for 3 timesteps.\n",
      "episode 54 score 0.03 average score 0.00 sum score 0.07 epsilon 0.91\n",
      "The episode ran for 2 timesteps.\n",
      "episode 55 score -0.01 average score 0.00 sum score 0.06 epsilon 0.91\n",
      "The episode ran for 8 timesteps.\n",
      "episode 56 score -0.02 average score 0.00 sum score 0.04 epsilon 0.91\n",
      "The episode ran for 4 timesteps.\n",
      "episode 57 score 0.02 average score 0.00 sum score 0.06 epsilon 0.91\n",
      "The episode ran for 5 timesteps.\n",
      "episode 58 score 0.03 average score 0.00 sum score 0.09 epsilon 0.90\n",
      "The episode ran for 8 timesteps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-de6f56bda782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2260b29bc737>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m                                      \u001b[0mstate_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                                      \u001b[0mstate_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                                      state_batch2[:])[batch_index, action_batch]\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         q_next = self.Q_eval.forward(new_state_batch[:,:,0], \n",
      "\u001b[1;32m<ipython-input-4-2260b29bc737>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input1, input2, input3, input4, input5, input6)\u001b[0m\n\u001b[0;32m     40\u001b[0m                           f1.view(f1.size(0), -1)), dim=1)\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from environment import environment\n",
    "\n",
    "historical_data = pd.read_csv(r'C:\\Users\\ratatosck\\Desktop\\pythonScripts\\TradeBot\\HistoricalData\\EURUSD15.csv', sep='\\t',header=None)\n",
    "historical_data.drop(0, axis=1, inplace = True)\n",
    "historical_data_np = historical_data.to_numpy(dtype = 'float32')\n",
    "\n",
    "### hyperparameters\n",
    "observation_size = 30\n",
    "\n",
    "env = environment(historical_data_np, observation_size)\n",
    "\n",
    "agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size=64, n_actions = 3, \n",
    "             eps_end = 0.01, input_dims1 = [observation_size, 5], input_dims2 = [2], lr=0.001)\n",
    "\n",
    "###\n",
    "scores, eps_history, all_scores_sum, all_scores_average = [], [], [], []\n",
    "n_games = 1000\n",
    "n_run = 10\n",
    "\n",
    "\n",
    "# the nn is not reseting on new run, youll have to decide if this is a good or bad thing.\n",
    "\n",
    "# also changing the graph to use the sum of scores (overall profit) after each game, instead of individual scores for each game\n",
    "# might be more clear as a metric for performance.\n",
    "\n",
    "for j in range(n_run):\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    for i in range (n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "#         avg_score = np.mean(scores[-100:])\n",
    "        avg_score = np.mean(scores)\n",
    "        sum_score = np.sum(scores)\n",
    "\n",
    "        print('episode', i, 'score %.2f' % score, \n",
    "                 'average score %.2f' % avg_score,\n",
    "                 'sum score %.2f' % sum_score,\n",
    "                 'epsilon %.2f'% agent.epsilon)\n",
    "        \n",
    "    all_scores_sum.append(sum(scores))\n",
    "    all_scores_average.append(sum(scores)/len(scores))\n",
    "        \n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    # filename = 'results-v2'\n",
    "    filename = 'test'\n",
    "    plot_learning_curve(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_scores_sum\n",
      "[-0.03840208053588867, -0.2329850196838379, 0.05150198936462402, 0.019414424896240234, 0.0834965705871582, 0.04280209541320801, -0.25120019912719727, -0.2958083152770996, 0.4400062561035156, 3.5171926021575928]\n",
      "all_scores_average\n",
      "[-0.001280069351196289, -0.007766167322794597, 0.0017167329788208008, 0.0006471474965413412, 0.00278321901957194, 0.0014267365137736003, -0.008373339970906576, -0.00986027717590332, 0.014666875203450521, 0.1172397534052531]\n"
     ]
    }
   ],
   "source": [
    "print(\"all_scores_sum\")\n",
    "print(all_scores_sum)\n",
    "print(\"all_scores_average\")\n",
    "print(all_scores_average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of problems right now. first, the training is'nt stable. This might be due to many things, there could be some catastrophic forgetting happening here, or it might just be the limit of the vanilla dqn, or it might just be that the environment is too random, or that the reward function is not good enough, maybe a bit of hyperparameter tuning might be good enough to solve the problem.\n",
    "\n",
    "There are a lot of questions that remain from this implementation: is the experience replay working? is the pipeline working? does the environment work? this will take a while to figure out.\n",
    "\n",
    "concerning the reward function, maybe giving negative reward when the agent is on the negative on a trade is good, enforcing early stopping through the reward function might be a good idea. \n",
    "\n",
    "also remember that as of now the reward scale is bound to increase as the portfolio increases, so you have to refactor it so that it is based not on profit but on percent return on investment.\n",
    "\n",
    "other solutions might involve implementing the target network, hyperparameter tuning; like the observation size, the replay memory size, the neural network size/ arquitecture, etc.\n",
    "\n",
    "however, there is no doubt that im going to have to try other algorithms.\n",
    "\n",
    "*I did the sum of the scores, and it seems that it ends up positive. this is an indicator that the algorithm might actually be working? I'll have to test and average over multiple runs to see if this is actually the case. thought this brings up an interesting question, I sort of expected the shape of the graph to be trending towards positive, but now that I think about it it seems obvious that it might never be the case, and it's not necessarily a bad thing, you cant expect the algorithm to have a positive trade every time, just like with people, it is normal to have bad trades, but its the average return that determines if the trader is proficient or not. And just like this, you cant expect an algorithm to always have positive returns. in conclusion, the metric for success is not only the shape of the graph, but the overall return from the whole training procedure; by doing the sum of all the scores \"sum(scores)\" and the higher the number  the better, and then doing an average over multiple training loops.\n",
    "\n",
    "*There is another idea that has to be investigated, should the agent be always \"greedy\" when deployed to production or should you always have a non zero epsilon? and should the agent still have a learning call or should you stop learning when the agent is deployed?\n",
    "\n",
    "*And finally the question of transfer learning, will it be possible? the training environment would have to be very close to the actual production environment for it to work. unless I come up with some revolutionary way to do transfer learning in rl (not likely).\n",
    "\n",
    "*another interesting idea might be to implement a sort of dual experience replay, that being 2 replay buffers, one for positive rewards and one for negative rewards and then sample from them equally. this might help with catastrophic forgetting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
