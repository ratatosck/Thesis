{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoInputsNet(nn.Module):\n",
    "    # ****\n",
    "    # tengo que revisar mejor sobre los channels y toda esta locura porque hay algo raro. sobretodo en la parte de los filtros.\n",
    "    \n",
    "    # revisa que todas tus dimensiones esten correctas.\n",
    "    \n",
    "    # puede ser que nn.sequential corre mejor que esta forma, tendre que ver si lo refactorizo. \n",
    "    # ****\n",
    "    def __init__(self, lr, in_channels, out_channels, kernel_size, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(TwoInputsNet, self).__init__()\n",
    "        self.conv_open = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_high = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_low = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_close = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv_volume = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.fc_state = nn.Linear(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims) #input dims is the sum of all outputs of the conv layers.\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.fc3 = nn.Linear(fc2_dims, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Open High Low Close Volume in that order (also for the numpy array)\n",
    "        # input6 son dos nodos de input para saber si el agente ha comprado/vendido y en cuanto.\n",
    "    def forward(self, input1, input2, input3, input4, input5, input6):\n",
    "        c1 = self.conv_open(input1.unsqueeze(dim = 1))\n",
    "        c2 = self.conv_high(input2.unsqueeze(dim = 1))\n",
    "        c3 = self.conv_low(input3.unsqueeze(dim = 1))\n",
    "        c4 = self.conv_close(input4.unsqueeze(dim = 1))\n",
    "        c5 = self.conv_volume(input5.unsqueeze(dim = 1))\n",
    "        f1 = self.fc_state(input6)\n",
    "        \n",
    "        # now we can reshape to 2D and concat them\n",
    "        combined = T.cat((c1.view(c1.size(0), -1), \n",
    "                          c2.view(c2.size(0), -1),\n",
    "                          c3.view(c3.size(0), -1),\n",
    "                          c4.view(c4.size(0), -1),\n",
    "                          c5.view(c5.size(0), -1),\n",
    "                          f1.view(f1.size(0), -1)), dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "class Agent():\n",
    "    # ***hyperparameters\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims1, input_dims2, batch_size, n_actions,\n",
    "                max_mem_size = 100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        # self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions, input_dims=input_dims, fc1_dims=256, fc2_dims=256)\n",
    "        # (self, lr, in_channels, out_channels, kernel_size, input_dims, fc1_dims, fc2_dims, n_actions)\n",
    "        # en otras parabras (learning rate, cantidad de filtros de entrada, cantidad de filtros de salida, tamaño del kernel)\n",
    "        #  ***hyperparameters\n",
    "        self.Q_eval = TwoInputsNet(self.lr, 1, 4, 3, 562, 128, 128, n_actions)\n",
    "        #                                             ^\n",
    "        # tengo que encontrar una forma para hacer que se haga el tamaño del fc layer automaticamente\n",
    "        \n",
    "        # ***\n",
    "        # tengo que reviar si el replay memory esta funcionando correctamente, sobre todo el *input_dims\n",
    "        # as of now I checked the state_memory shape (10000, 30, 5) and it looks correct\n",
    "        # ***\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims1), dtype = np.float32)\n",
    "        self.state_memory2 = np.zeros((self.mem_size, *input_dims2), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims1), dtype=np.float32)\n",
    "        self.new_state_memory2 = np.zeros((self.mem_size, *input_dims2), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state[0]\n",
    "        self.state_memory2[index] = state[1]\n",
    "        self.new_state_memory[index] = state_[0]\n",
    "        self.new_state_memory2[index] = state_[1]\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # aqui tuve que separar la observacion en las 2 partes y transformarlas a pytorch tensor. \n",
    "            # no se si es lo mejor, pero sirve asi que...\n",
    "            \n",
    "            state = T.tensor([observation[0]]).to(self.Q_eval.device)\n",
    "            state2 = T.tensor([observation[1]]).to(self.Q_eval.device)\n",
    "#             print(state)\n",
    "            actions = self.Q_eval.forward(state[:,:,0], \n",
    "                                          state[:,:,1], \n",
    "                                          state[:,:,2], \n",
    "                                          state[:,:,3], \n",
    "                                          state[:,:,4],\n",
    "                                          state2[:])\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        state_batch2 = T.tensor(self.state_memory2[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch2 = T.tensor(self.new_state_memory2[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch[:,:,0],\n",
    "                                     state_batch[:,:,1],\n",
    "                                     state_batch[:,:,2],\n",
    "                                     state_batch[:,:,3],\n",
    "                                     state_batch[:,:,4],\n",
    "                                     state_batch2[:])[batch_index, action_batch]\n",
    "    \n",
    "        q_next = self.Q_eval.forward(new_state_batch[:,:,0],\n",
    "                                     new_state_batch[:,:,1],\n",
    "                                     new_state_batch[:,:,2],\n",
    "                                     new_state_batch[:,:,3],\n",
    "                                     new_state_batch[:,:,4],\n",
    "                                     new_state_batch2[:])\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                        else self.eps_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environment import environment\n",
    "\n",
    "historical_data = pd.read_csv(r'C:\\Users\\ratatosck\\Desktop\\pythonScripts\\TradeBot\\HistoricalData\\EURUSD15.csv', sep='\\t',header=None)\n",
    "historical_data.drop(0, axis=1, inplace = True)\n",
    "historical_data_np = historical_data.to_numpy(dtype = 'float32')\n",
    "\n",
    "### hyperparameters\n",
    "observation_size = 30\n",
    "\n",
    "# env = environment(historical_data_np, observation_size)\n",
    "\n",
    "# agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size=64, n_actions = 3, \n",
    "#              eps_end = 0.01, input_dims1 = [observation_size, 5], input_dims2 = [2], lr=0.001)\n",
    "\n",
    "###\n",
    "scores, eps_history, all_scores_sum, all_scores_average = [], [], [], []\n",
    "n_games = 1000\n",
    "n_run = 10\n",
    "\n",
    "\n",
    "# the nn is not reseting on new run, youll have to decide if this is a good or bad thing.\n",
    "\n",
    "# also changing the graph to use the sum of scores (overall profit) after each episode, instead of individual scores for \n",
    "# each episode might be more clear as a metric for performance.\n",
    "\n",
    "for j in range(n_run):\n",
    "    scores = []\n",
    "    sum_scores = []\n",
    "    avg_scores = []\n",
    "    eps_history = []\n",
    "    \n",
    "    env = environment(historical_data_np, observation_size)\n",
    "    agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size=64, n_actions = 3, \n",
    "             eps_end = 0.01, input_dims1 = [observation_size, 5], input_dims2 = [2], lr=0.001)\n",
    "    \n",
    "    for i in range (n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "#         avg_score = np.mean(scores[-100:])\n",
    "        avg_score = np.mean(scores)\n",
    "        sum_score = np.sum(scores)\n",
    "        avg_scores.append(avg_score)\n",
    "        sum_scores.append(sum_score)\n",
    "\n",
    "        print('episode', i, 'score %.2f' % score, \n",
    "                 'average score %.2f' % avg_score,\n",
    "                 'sum score %.2f' % sum_score,\n",
    "                 'epsilon %.2f'% agent.epsilon)\n",
    "        \n",
    "    all_scores_sum.append(sum(scores))\n",
    "    all_scores_average.append(sum(scores)/len(scores))\n",
    "        \n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    # filename = 'results-v2'\n",
    "    \n",
    "\n",
    "    filename1 = 'plots/score-graph-' + str(j+1)\n",
    "    filename2 = 'plots/sum-graph-' + str(j+1)\n",
    "    filename3 = 'plots/average-graph-' + str(j+1)\n",
    "    plot_learning_curve(x, scores, eps_history, filename1)\n",
    "    plot_learning_curve(x, sum_scores, eps_history, filename2)\n",
    "    plot_learning_curve(x, avg_scores, eps_history, filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_scores_sum\n",
      "[142.50312328338623, 10.277478694915771, 95.39838194847107, 575.3482830524445, 2.6060569286346436, 846.4772915840149, -13.758081197738647, 83.10495138168335, 115.80050230026245, 359.78622794151306]\n",
      "all_scores_average\n",
      "[0.14250312328338624, 0.010277478694915772, 0.09539838194847107, 0.5753482830524445, 0.0026060569286346434, 0.8464772915840149, -0.013758081197738647, 0.08310495138168335, 0.11580050230026245, 0.35978622794151305]\n"
     ]
    }
   ],
   "source": [
    "print(\"all_scores_sum\")\n",
    "print(all_scores_sum)\n",
    "print(\"all_scores_average\")\n",
    "print(all_scores_average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of problems right now. first, the training is'nt stable. This might be due to many things, there could be some catastrophic forgetting happening here, or it might just be the limit of the vanilla dqn, or it might just be that the environment is too random, or that the reward function is not good enough, maybe a bit of hyperparameter tuning might be good enough to solve the problem.\n",
    "\n",
    "There are a lot of questions that remain from this implementation: is the experience replay working? is the pipeline working? does the environment work? this will take a while to figure out.\n",
    "\n",
    "concerning the reward function, maybe giving negative reward when the agent is on the negative on a trade is good, enforcing early stopping through the reward function might be a good idea. \n",
    "\n",
    "also remember that as of now the reward scale is bound to increase as the portfolio increases, so you have to refactor it so that it is based not on profit but on percent return on investment.\n",
    "\n",
    "other solutions might involve implementing the target network, hyperparameter tuning; like the observation size, the replay memory size, the neural network size/ arquitecture, etc.\n",
    "\n",
    "however, there is no doubt that im going to have to try other algorithms.\n",
    "\n",
    "*I did the sum of the scores, and it seems that it ends up positive. this is an indicator that the algorithm might actually be working? I'll have to test and average over multiple runs to see if this is actually the case. thought this brings up an interesting question, I sort of expected the shape of the graph to be trending towards positive, but now that I think about it it seems obvious that it might never be the case, and it's not necessarily a bad thing, you cant expect the algorithm to have a positive trade every time, just like with people, it is normal to have bad trades, but its the average return that determines if the trader is proficient or not. And just like this, you cant expect an algorithm to always have positive returns. in conclusion, the metric for success is not only the shape of the graph, but the overall return from the whole training procedure; by doing the sum of all the scores \"sum(scores)\" and the higher the number  the better, and then doing an average over multiple training loops.\n",
    "\n",
    "*There is another idea that has to be investigated, should the agent be always \"greedy\" when deployed to production or should you always have a non zero epsilon? and should the agent still have a learning call or should you stop learning when the agent is deployed?\n",
    "\n",
    "*And finally the question of transfer learning, will it be possible? the training environment would have to be very close to the actual production environment for it to work. unless I come up with some revolutionary way to do transfer learning in rl (not likely).\n",
    "\n",
    "*another interesting idea might be to implement a sort of dual experience replay, that being 2 replay buffers, one for positive rewards and one for negative rewards and then sample from them equally. this might help with catastrophic forgetting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
