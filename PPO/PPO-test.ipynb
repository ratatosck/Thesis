{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype = np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "    \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "        \n",
    "        \n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha, \n",
    "                 fc1_dims = 256, fc2_dims = 256, chkpt_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "#         print('model dist')\n",
    "#         print(dist)\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims = 256, fc2_dims = 256, \n",
    "                chkpt_dir = 'tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                    nn.Linear(*input_dims, fc1_dims),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(fc1_dims, fc2_dims),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(fc2_dims, 1)\n",
    "            )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "        \n",
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "                policy_clip=0.1, batch_size=64, N=2048, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "        \n",
    "    def save_models(self):\n",
    "        print('saving models')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        \n",
    "    def load_models(self):\n",
    "        print('loading models')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "#         print(\"printing dist\")\n",
    "#         print(dist)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "        \n",
    "        return action, probs, value\n",
    "    \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "                reward_arr, dones_arr, batches= self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "            \n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                                    (1-int(dones_arr[k])) - values[k])\n",
    "                    \n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "            \n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "                \n",
    "#                 print(\"states\")\n",
    "#                 print(states.shape)\n",
    "#                 print(states)\n",
    "                \n",
    "                dist = self.actor(states)\n",
    "                \n",
    "#                 print('distribution')\n",
    "#                 print(dist)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                \n",
    "#                 print('critic_value')\n",
    "#                 print(critic_value)\n",
    "                \n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                \n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, \n",
    "                                                1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "                \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "            \n",
    "        self.memory.clear_memory()\n",
    "        \n",
    "        \n",
    "    \n",
    "                                            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving models\n",
      "episode 0 score 25.0 avg score 25.0 time_steps 25 learning_steps 1\n",
      "episode 1 score 17.0 avg score 21.0 time_steps 42 learning_steps 2\n",
      "saving models\n",
      "episode 2 score 44.0 avg score 28.7 time_steps 86 learning_steps 4\n",
      "saving models\n",
      "episode 3 score 37.0 avg score 30.8 time_steps 123 learning_steps 6\n",
      "episode 4 score 17.0 avg score 28.0 time_steps 140 learning_steps 7\n",
      "saving models\n",
      "episode 5 score 60.0 avg score 33.3 time_steps 200 learning_steps 10\n",
      "episode 6 score 29.0 avg score 32.7 time_steps 229 learning_steps 11\n",
      "saving models\n",
      "episode 7 score 72.0 avg score 37.6 time_steps 301 learning_steps 15\n",
      "saving models\n",
      "episode 8 score 50.0 avg score 39.0 time_steps 351 learning_steps 17\n",
      "saving models\n",
      "episode 9 score 61.0 avg score 41.2 time_steps 412 learning_steps 20\n",
      "episode 10 score 41.0 avg score 41.2 time_steps 453 learning_steps 22\n",
      "episode 11 score 14.0 avg score 38.9 time_steps 467 learning_steps 23\n",
      "episode 12 score 51.0 avg score 39.8 time_steps 518 learning_steps 25\n",
      "episode 13 score 19.0 avg score 38.4 time_steps 537 learning_steps 26\n",
      "saving models\n",
      "episode 14 score 94.0 avg score 42.1 time_steps 631 learning_steps 31\n",
      "saving models\n",
      "episode 15 score 75.0 avg score 44.1 time_steps 706 learning_steps 35\n",
      "saving models\n",
      "episode 16 score 85.0 avg score 46.5 time_steps 791 learning_steps 39\n",
      "saving models\n",
      "episode 17 score 127.0 avg score 51.0 time_steps 918 learning_steps 45\n",
      "episode 18 score 39.0 avg score 50.4 time_steps 957 learning_steps 47\n",
      "episode 19 score 54.0 avg score 50.5 time_steps 1011 learning_steps 50\n",
      "episode 20 score 21.0 avg score 49.1 time_steps 1032 learning_steps 51\n",
      "episode 21 score 73.0 avg score 50.2 time_steps 1105 learning_steps 55\n",
      "episode 22 score 46.0 avg score 50.0 time_steps 1151 learning_steps 57\n",
      "episode 23 score 72.0 avg score 51.0 time_steps 1223 learning_steps 61\n",
      "episode 24 score 44.0 avg score 50.7 time_steps 1267 learning_steps 63\n",
      "saving models\n",
      "episode 25 score 82.0 avg score 51.9 time_steps 1349 learning_steps 67\n",
      "saving models\n",
      "episode 26 score 62.0 avg score 52.3 time_steps 1411 learning_steps 70\n",
      "saving models\n",
      "episode 27 score 200.0 avg score 57.5 time_steps 1611 learning_steps 80\n",
      "saving models\n",
      "episode 28 score 91.0 avg score 58.7 time_steps 1702 learning_steps 85\n",
      "saving models\n",
      "episode 29 score 79.0 avg score 59.4 time_steps 1781 learning_steps 89\n",
      "episode 30 score 58.0 avg score 59.3 time_steps 1839 learning_steps 91\n",
      "episode 31 score 28.0 avg score 58.3 time_steps 1867 learning_steps 93\n",
      "episode 32 score 33.0 avg score 57.6 time_steps 1900 learning_steps 95\n",
      "episode 33 score 69.0 avg score 57.9 time_steps 1969 learning_steps 98\n",
      "episode 34 score 33.0 avg score 57.2 time_steps 2002 learning_steps 100\n",
      "episode 35 score 31.0 avg score 56.5 time_steps 2033 learning_steps 101\n",
      "episode 36 score 38.0 avg score 56.0 time_steps 2071 learning_steps 103\n",
      "episode 37 score 48.0 avg score 55.8 time_steps 2119 learning_steps 105\n",
      "episode 38 score 85.0 avg score 56.5 time_steps 2204 learning_steps 110\n",
      "saving models\n",
      "episode 39 score 200.0 avg score 60.1 time_steps 2404 learning_steps 120\n",
      "episode 40 score 34.0 avg score 59.5 time_steps 2438 learning_steps 121\n",
      "episode 41 score 42.0 avg score 59.0 time_steps 2480 learning_steps 124\n",
      "episode 42 score 66.0 avg score 59.2 time_steps 2546 learning_steps 127\n",
      "saving models\n",
      "episode 43 score 108.0 avg score 60.3 time_steps 2654 learning_steps 132\n",
      "saving models\n",
      "episode 44 score 126.0 avg score 61.8 time_steps 2780 learning_steps 139\n",
      "saving models\n",
      "episode 45 score 121.0 avg score 63.1 time_steps 2901 learning_steps 145\n",
      "episode 46 score 37.0 avg score 62.5 time_steps 2938 learning_steps 146\n",
      "episode 47 score 22.0 avg score 61.7 time_steps 2960 learning_steps 148\n",
      "episode 48 score 101.0 avg score 62.5 time_steps 3061 learning_steps 153\n",
      "saving models\n",
      "episode 49 score 98.0 avg score 63.2 time_steps 3159 learning_steps 157\n",
      "saving models\n",
      "episode 50 score 168.0 avg score 65.2 time_steps 3327 learning_steps 166\n",
      "episode 51 score 27.0 avg score 64.5 time_steps 3354 learning_steps 167\n",
      "saving models\n",
      "episode 52 score 200.0 avg score 67.1 time_steps 3554 learning_steps 177\n",
      "saving models\n",
      "episode 53 score 200.0 avg score 69.5 time_steps 3754 learning_steps 187\n",
      "episode 54 score 41.0 avg score 69.0 time_steps 3795 learning_steps 189\n",
      "episode 55 score 20.0 avg score 68.1 time_steps 3815 learning_steps 190\n",
      "episode 56 score 27.0 avg score 67.4 time_steps 3842 learning_steps 192\n",
      "saving models\n",
      "episode 57 score 200.0 avg score 69.7 time_steps 4042 learning_steps 202\n",
      "saving models\n",
      "episode 58 score 200.0 avg score 71.9 time_steps 4242 learning_steps 212\n",
      "saving models\n",
      "episode 59 score 112.0 avg score 72.6 time_steps 4354 learning_steps 217\n",
      "saving models\n",
      "episode 60 score 191.0 avg score 74.5 time_steps 4545 learning_steps 227\n",
      "saving models\n",
      "episode 61 score 142.0 avg score 75.6 time_steps 4687 learning_steps 234\n",
      "saving models\n",
      "episode 62 score 200.0 avg score 77.6 time_steps 4887 learning_steps 244\n",
      "saving models\n",
      "episode 63 score 158.0 avg score 78.8 time_steps 5045 learning_steps 252\n",
      "saving models\n",
      "episode 64 score 200.0 avg score 80.7 time_steps 5245 learning_steps 262\n",
      "saving models\n",
      "episode 65 score 113.0 avg score 81.2 time_steps 5358 learning_steps 267\n",
      "saving models\n",
      "episode 66 score 124.0 avg score 81.8 time_steps 5482 learning_steps 274\n",
      "saving models\n",
      "episode 67 score 105.0 avg score 82.2 time_steps 5587 learning_steps 279\n",
      "saving models\n",
      "episode 68 score 129.0 avg score 82.8 time_steps 5716 learning_steps 285\n",
      "saving models\n",
      "episode 69 score 137.0 avg score 83.6 time_steps 5853 learning_steps 292\n",
      "saving models\n",
      "episode 70 score 134.0 avg score 84.3 time_steps 5987 learning_steps 299\n",
      "saving models\n",
      "episode 71 score 105.0 avg score 84.6 time_steps 6092 learning_steps 304\n",
      "saving models\n",
      "episode 72 score 113.0 avg score 85.0 time_steps 6205 learning_steps 310\n",
      "saving models\n",
      "episode 73 score 135.0 avg score 85.7 time_steps 6340 learning_steps 317\n",
      "saving models\n",
      "episode 74 score 198.0 avg score 87.2 time_steps 6538 learning_steps 326\n",
      "episode 75 score 38.0 avg score 86.5 time_steps 6576 learning_steps 328\n",
      "saving models\n",
      "episode 76 score 200.0 avg score 88.0 time_steps 6776 learning_steps 338\n",
      "saving models\n",
      "episode 77 score 200.0 avg score 89.4 time_steps 6976 learning_steps 348\n",
      "saving models\n",
      "episode 78 score 192.0 avg score 90.7 time_steps 7168 learning_steps 358\n",
      "saving models\n",
      "episode 79 score 155.0 avg score 91.5 time_steps 7323 learning_steps 366\n",
      "saving models\n",
      "episode 80 score 200.0 avg score 92.9 time_steps 7523 learning_steps 376\n",
      "saving models\n",
      "episode 81 score 200.0 avg score 94.2 time_steps 7723 learning_steps 386\n",
      "saving models\n",
      "episode 82 score 200.0 avg score 95.5 time_steps 7923 learning_steps 396\n",
      "saving models\n",
      "episode 83 score 200.0 avg score 96.7 time_steps 8123 learning_steps 406\n",
      "saving models\n",
      "episode 84 score 200.0 avg score 97.9 time_steps 8323 learning_steps 416\n",
      "saving models\n",
      "episode 85 score 144.0 avg score 98.5 time_steps 8467 learning_steps 423\n",
      "saving models\n",
      "episode 86 score 200.0 avg score 99.6 time_steps 8667 learning_steps 433\n",
      "saving models\n",
      "episode 87 score 200.0 avg score 100.8 time_steps 8867 learning_steps 443\n",
      "saving models\n",
      "episode 88 score 200.0 avg score 101.9 time_steps 9067 learning_steps 453\n",
      "saving models\n",
      "episode 89 score 200.0 avg score 103.0 time_steps 9267 learning_steps 463\n",
      "saving models\n",
      "episode 90 score 200.0 avg score 104.0 time_steps 9467 learning_steps 473\n",
      "saving models\n",
      "episode 91 score 200.0 avg score 105.1 time_steps 9667 learning_steps 483\n",
      "saving models\n",
      "episode 92 score 200.0 avg score 106.1 time_steps 9867 learning_steps 493\n",
      "saving models\n",
      "episode 93 score 200.0 avg score 107.1 time_steps 10067 learning_steps 503\n",
      "saving models\n",
      "episode 94 score 149.0 avg score 107.5 time_steps 10216 learning_steps 510\n",
      "episode 95 score 76.0 avg score 107.2 time_steps 10292 learning_steps 514\n",
      "episode 96 score 88.0 avg score 107.0 time_steps 10380 learning_steps 519\n",
      "episode 97 score 36.0 avg score 106.3 time_steps 10416 learning_steps 520\n",
      "episode 98 score 23.0 avg score 105.4 time_steps 10439 learning_steps 521\n",
      "episode 99 score 36.0 avg score 104.8 time_steps 10475 learning_steps 523\n",
      "episode 100 score 29.0 avg score 104.8 time_steps 10504 learning_steps 525\n",
      "episode 101 score 24.0 avg score 104.9 time_steps 10528 learning_steps 526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 102 score 31.0 avg score 104.7 time_steps 10559 learning_steps 527\n",
      "episode 103 score 106.0 avg score 105.4 time_steps 10665 learning_steps 533\n",
      "episode 104 score 31.0 avg score 105.6 time_steps 10696 learning_steps 534\n",
      "episode 105 score 42.0 avg score 105.4 time_steps 10738 learning_steps 536\n",
      "episode 106 score 112.0 avg score 106.2 time_steps 10850 learning_steps 542\n",
      "episode 107 score 117.0 avg score 106.7 time_steps 10967 learning_steps 548\n",
      "episode 108 score 102.0 avg score 107.2 time_steps 11069 learning_steps 553\n",
      "saving models\n",
      "episode 109 score 126.0 avg score 107.8 time_steps 11195 learning_steps 559\n",
      "saving models\n",
      "episode 110 score 126.0 avg score 108.7 time_steps 11321 learning_steps 566\n",
      "saving models\n",
      "episode 111 score 127.0 avg score 109.8 time_steps 11448 learning_steps 572\n",
      "saving models\n",
      "episode 112 score 185.0 avg score 111.2 time_steps 11633 learning_steps 581\n",
      "saving models\n",
      "episode 113 score 135.0 avg score 112.3 time_steps 11768 learning_steps 588\n",
      "saving models\n",
      "episode 114 score 166.0 avg score 113.0 time_steps 11934 learning_steps 596\n",
      "saving models\n",
      "episode 115 score 192.0 avg score 114.2 time_steps 12126 learning_steps 606\n",
      "saving models\n",
      "episode 116 score 161.0 avg score 115.0 time_steps 12287 learning_steps 614\n",
      "saving models\n",
      "episode 117 score 172.0 avg score 115.4 time_steps 12459 learning_steps 622\n",
      "saving models\n",
      "episode 118 score 200.0 avg score 117.0 time_steps 12659 learning_steps 632\n",
      "saving models\n",
      "episode 119 score 200.0 avg score 118.5 time_steps 12859 learning_steps 642\n",
      "saving models\n",
      "episode 120 score 200.0 avg score 120.3 time_steps 13059 learning_steps 652\n",
      "saving models\n",
      "episode 121 score 200.0 avg score 121.5 time_steps 13259 learning_steps 662\n",
      "saving models\n",
      "episode 122 score 200.0 avg score 123.1 time_steps 13459 learning_steps 672\n",
      "saving models\n",
      "episode 123 score 200.0 avg score 124.4 time_steps 13659 learning_steps 682\n",
      "saving models\n",
      "episode 124 score 200.0 avg score 125.9 time_steps 13859 learning_steps 692\n",
      "saving models\n",
      "episode 125 score 108.0 avg score 126.2 time_steps 13967 learning_steps 698\n",
      "saving models\n",
      "episode 126 score 200.0 avg score 127.6 time_steps 14167 learning_steps 708\n",
      "episode 127 score 200.0 avg score 127.6 time_steps 14367 learning_steps 718\n",
      "saving models\n",
      "episode 128 score 200.0 avg score 128.7 time_steps 14567 learning_steps 728\n",
      "saving models\n",
      "episode 129 score 200.0 avg score 129.9 time_steps 14767 learning_steps 738\n",
      "saving models\n",
      "episode 130 score 200.0 avg score 131.3 time_steps 14967 learning_steps 748\n",
      "saving models\n",
      "episode 131 score 187.0 avg score 132.9 time_steps 15154 learning_steps 757\n",
      "saving models\n",
      "episode 132 score 142.0 avg score 134.0 time_steps 15296 learning_steps 764\n",
      "saving models\n",
      "episode 133 score 154.0 avg score 134.8 time_steps 15450 learning_steps 772\n",
      "saving models\n",
      "episode 134 score 181.0 avg score 136.3 time_steps 15631 learning_steps 781\n",
      "saving models\n",
      "episode 135 score 155.0 avg score 137.5 time_steps 15786 learning_steps 789\n",
      "saving models\n",
      "episode 136 score 200.0 avg score 139.2 time_steps 15986 learning_steps 799\n",
      "saving models\n",
      "episode 137 score 196.0 avg score 140.6 time_steps 16182 learning_steps 809\n",
      "saving models\n",
      "episode 138 score 200.0 avg score 141.8 time_steps 16382 learning_steps 819\n",
      "episode 139 score 200.0 avg score 141.8 time_steps 16582 learning_steps 829\n",
      "saving models\n",
      "episode 140 score 153.0 avg score 143.0 time_steps 16735 learning_steps 836\n",
      "saving models\n",
      "episode 141 score 168.0 avg score 144.2 time_steps 16903 learning_steps 845\n",
      "saving models\n",
      "episode 142 score 169.0 avg score 145.3 time_steps 17072 learning_steps 853\n",
      "saving models\n",
      "episode 143 score 200.0 avg score 146.2 time_steps 17272 learning_steps 863\n",
      "saving models\n",
      "episode 144 score 200.0 avg score 146.9 time_steps 17472 learning_steps 873\n",
      "saving models\n",
      "episode 145 score 191.0 avg score 147.6 time_steps 17663 learning_steps 883\n",
      "saving models\n",
      "episode 146 score 200.0 avg score 149.2 time_steps 17863 learning_steps 893\n",
      "saving models\n",
      "episode 147 score 200.0 avg score 151.0 time_steps 18063 learning_steps 903\n",
      "saving models\n",
      "episode 148 score 200.0 avg score 152.0 time_steps 18263 learning_steps 913\n",
      "saving models\n",
      "episode 149 score 200.0 avg score 153.0 time_steps 18463 learning_steps 923\n",
      "saving models\n",
      "episode 150 score 200.0 avg score 153.4 time_steps 18663 learning_steps 933\n",
      "saving models\n",
      "episode 151 score 200.0 avg score 155.1 time_steps 18863 learning_steps 943\n",
      "episode 152 score 200.0 avg score 155.1 time_steps 19063 learning_steps 953\n",
      "episode 153 score 200.0 avg score 155.1 time_steps 19263 learning_steps 963\n",
      "saving models\n",
      "episode 154 score 200.0 avg score 156.7 time_steps 19463 learning_steps 973\n",
      "saving models\n",
      "episode 155 score 164.0 avg score 158.1 time_steps 19627 learning_steps 981\n",
      "saving models\n",
      "episode 156 score 172.0 avg score 159.6 time_steps 19799 learning_steps 989\n",
      "episode 157 score 114.0 avg score 158.7 time_steps 19913 learning_steps 995\n",
      "episode 158 score 200.0 avg score 158.7 time_steps 20113 learning_steps 1005\n",
      "saving models\n",
      "episode 159 score 200.0 avg score 159.6 time_steps 20313 learning_steps 1015\n",
      "saving models\n",
      "episode 160 score 200.0 avg score 159.7 time_steps 20513 learning_steps 1025\n",
      "saving models\n",
      "episode 161 score 200.0 avg score 160.3 time_steps 20713 learning_steps 1035\n",
      "episode 162 score 200.0 avg score 160.3 time_steps 20913 learning_steps 1045\n",
      "saving models\n",
      "episode 163 score 200.0 avg score 160.7 time_steps 21113 learning_steps 1055\n",
      "episode 164 score 200.0 avg score 160.7 time_steps 21313 learning_steps 1065\n",
      "saving models\n",
      "episode 165 score 200.0 avg score 161.6 time_steps 21513 learning_steps 1075\n",
      "saving models\n",
      "episode 166 score 200.0 avg score 162.3 time_steps 21713 learning_steps 1085\n",
      "saving models\n",
      "episode 167 score 200.0 avg score 163.3 time_steps 21913 learning_steps 1095\n",
      "saving models\n",
      "episode 168 score 200.0 avg score 164.0 time_steps 22113 learning_steps 1105\n",
      "saving models\n",
      "episode 169 score 200.0 avg score 164.6 time_steps 22313 learning_steps 1115\n",
      "saving models\n",
      "episode 170 score 200.0 avg score 165.3 time_steps 22513 learning_steps 1125\n",
      "saving models\n",
      "episode 171 score 200.0 avg score 166.2 time_steps 22713 learning_steps 1135\n",
      "saving models\n",
      "episode 172 score 200.0 avg score 167.1 time_steps 22913 learning_steps 1145\n",
      "saving models\n",
      "episode 173 score 200.0 avg score 167.7 time_steps 23113 learning_steps 1155\n",
      "saving models\n",
      "episode 174 score 200.0 avg score 167.8 time_steps 23313 learning_steps 1165\n",
      "saving models\n",
      "episode 175 score 200.0 avg score 169.4 time_steps 23513 learning_steps 1175\n",
      "episode 176 score 200.0 avg score 169.4 time_steps 23713 learning_steps 1185\n",
      "episode 177 score 200.0 avg score 169.4 time_steps 23913 learning_steps 1195\n",
      "episode 178 score 190.0 avg score 169.3 time_steps 24103 learning_steps 1205\n",
      "saving models\n",
      "episode 179 score 182.0 avg score 169.6 time_steps 24285 learning_steps 1214\n",
      "episode 180 score 193.0 avg score 169.6 time_steps 24478 learning_steps 1223\n",
      "episode 181 score 160.0 avg score 169.2 time_steps 24638 learning_steps 1231\n",
      "episode 182 score 200.0 avg score 169.2 time_steps 24838 learning_steps 1241\n",
      "episode 183 score 195.0 avg score 169.1 time_steps 25033 learning_steps 1251\n",
      "episode 184 score 200.0 avg score 169.1 time_steps 25233 learning_steps 1261\n",
      "saving models\n",
      "episode 185 score 200.0 avg score 169.7 time_steps 25433 learning_steps 1271\n",
      "episode 186 score 200.0 avg score 169.7 time_steps 25633 learning_steps 1281\n",
      "episode 187 score 200.0 avg score 169.7 time_steps 25833 learning_steps 1291\n",
      "episode 188 score 200.0 avg score 169.7 time_steps 26033 learning_steps 1301\n",
      "episode 189 score 200.0 avg score 169.7 time_steps 26233 learning_steps 1311\n",
      "episode 190 score 200.0 avg score 169.7 time_steps 26433 learning_steps 1321\n",
      "episode 191 score 200.0 avg score 169.7 time_steps 26633 learning_steps 1331\n",
      "episode 192 score 200.0 avg score 169.7 time_steps 26833 learning_steps 1341\n",
      "episode 193 score 200.0 avg score 169.7 time_steps 27033 learning_steps 1351\n",
      "saving models\n",
      "episode 194 score 200.0 avg score 170.2 time_steps 27233 learning_steps 1361\n",
      "saving models\n",
      "episode 195 score 200.0 avg score 171.4 time_steps 27433 learning_steps 1371\n",
      "saving models\n",
      "episode 196 score 200.0 avg score 172.5 time_steps 27633 learning_steps 1381\n",
      "saving models\n",
      "episode 197 score 200.0 avg score 174.2 time_steps 27833 learning_steps 1391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving models\n",
      "episode 198 score 200.0 avg score 175.9 time_steps 28033 learning_steps 1401\n",
      "saving models\n",
      "episode 199 score 200.0 avg score 177.6 time_steps 28233 learning_steps 1411\n",
      "saving models\n",
      "episode 200 score 200.0 avg score 179.3 time_steps 28433 learning_steps 1421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fe53c9ace67a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-eba002f89e75>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;31m#         print(\"printing dist\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-eba002f89e75>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from utils import plot_learning_curve\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.0003\n",
    "agent = Agent(n_actions=env.action_space.n, batch_size = batch_size,\n",
    "             alpha = alpha, n_epochs = n_epochs, \n",
    "              input_dims = env.observation_space.shape)\n",
    "\n",
    "n_games = 300\n",
    "\n",
    "figure_file = 'plots/cartpole.png'\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "\n",
    "    print('episode', i, 'score %.1f' %score, 'avg score %.1f' %avg_score, \n",
    "          'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "        \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "plot_learning_curve(x, score_history, figure_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
